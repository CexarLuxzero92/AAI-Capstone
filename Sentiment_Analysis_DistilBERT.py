# -*- coding: utf-8 -*-
"""Model2BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18aJtA595pOTbrK8WMQPdDtJWNM-_S2Bc
"""

import spacy
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from google.colab import drive
import requests
import os
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from transformers import DistilBertTokenizer, AutoTokenizer

df_preprocessed = pd.read_csv('processed.csv')
df_preprocessed.head(10)

model_ckpt = "distilbert-base-uncased"
tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)
tokenizer.model_input_names

df_preprocessed['lemmas'] = df_preprocessed['lemmas'].astype(str)

length = df_preprocessed['tokens'].apply(len)
print(length.describe())

plt.hist(length, bins=50, alpha=0.7)
plt.title('Distribution of Sequence Lengths')
plt.xlabel('Sequence Length')
plt.ylabel('Frequency')
plt.show()

def tokenize(text, max_length=200):
  return tokenizer(text, padding='max_length', truncation=True)

print(tokenize(df_preprocessed['lemmas'][5]))

df_preprocessed['input_ids'] = None
df_preprocessed['attention_mask'] = None

for index, row in df_preprocessed.iterrows():
    tokenized_output = tokenize(row['lemmas'])
    df_preprocessed.at[index, 'input_ids'] = tokenized_output['input_ids']
    df_preprocessed.at[index, 'attention_mask'] = tokenized_output['attention_mask']

df_preprocessed.head(10)

label_list = df_preprocessed['emotion_label'].unique()
num_labels = len(label_list)
print('Labels: ',label_list, '\n Qty:', num_labels)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(df_preprocessed['emotion_label'])

emotions_encoded = pd.DataFrame(columns=['attention_mask', 'input_ids', 'label',
                                         'text' ])
emotions_encoded['attention_mask'] = df_preprocessed['attention_mask']
emotions_encoded['input_ids'] = df_preprocessed['input_ids']
emotions_encoded['label'] = labels
emotions_encoded['text'] = df_preprocessed['cleaned_text']

emotions_encoded

"""Aqui tenemos que hacer train and test split, pero que sea stratified en todo el dataset, el emotions_encoded ya esta listo para procesarse, solo hayq ue hacer el split, entrenar el model y rezarle a Turing"""

emotions_train = emotions_encoded

emotions_encoded.set_format("torch", columns=["input_ids", "attention_mask",
                                              "label"])

import torch
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification
device = torch.device("cpu")
model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device))

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels, preds, average="weighted")
  acc = accuracy_score(labels, preds)
  return {"accuracy": acc, "f1": f1}

!pip install --quiet accelerate -U

!pip install --quiet transformers[torch]

from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(emotions_encoded) // batch_size
model_name = f"{model_ckpt}-finetuned-emotion"
training_args = TrainingArguments(output_dir=model_name, num_train_epochs=2,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01, evaluation_strategy="epoch",
                                  disable_tqdm=False, logging_steps=logging_steps,
                                  push_to_hub=True, log_level="error")

from transformers import Trainer
trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=emotions_encoded["train"],
                  eval_dataset=emotions_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train();

