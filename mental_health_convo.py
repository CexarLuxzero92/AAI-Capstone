# -*- coding: utf-8 -*-
"""Mental_health_convo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXaFU31abkpXvvN5hG9o-EQDG-cgz6Tp

# Data 1
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations

## Imports
"""

import pandas as pd
import re
from google.colab import files

# libraries for the files in google drive
from pydrive.auth import GoogleAuth
from google.colab import drive
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Visualizations
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import matplotlib.pyplot as plt

"""## Explore data"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_id = '1uMXnIYkTLw1A9ExUCrhkXQCUO0whTmAs' # id from you google drive file

download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('train.csv')
df  = pd.read_csv("train.csv")
df.head()

df.shape

df.info()

df.isnull().sum()

"""## Clean data"""

#@title Drop null rows
null_rows = df.isnull().any(axis=1) # Identify rows with null values
df_cleaned = df[~null_rows] # Drop rows with null values

df_cleaned.shape # Display new shape

#@title Remove the duplicate rows
df.drop_duplicates()
df.shape

df["Context"][0]

df["Response"][0]

#@title Normalize Text
# & Remove Unicode Characters

def clean_text(text):
  text = str(text).lower() # Convert text to lowercase
  text = re.sub("[\[].*?[\]]", "", text) # Remove text within square brackets
  text = re.sub(r"http\S+", "", text) # Remove URLs
  text = re.sub(r"[^\w\s]", "", text) # Remove non-alphanumeric characters (excluding spaces)
  text = re.sub(r"\d+", "", text) # Remove digits
  return text

# Apply the clean_text function
df['Context'].apply(clean_text)
df['Response'].apply(clean_text)
df.head()

#@title Remove escape sequences

df['Context'] = df['Context'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())
df['Response'] = df['Response'].astype(str).apply(lambda x: re.sub(r'\s+', ' ', x).strip())

df.head()

df["Context"][0]

df["Response"][0]

"""## Add token"""

df['Context'] = df['Context'].apply(lambda x: "<user> " + str(x))
df['Response'] = df['Response'].apply(lambda x: "<agent> " + str(x))

df.head()

"""## Visualizations"""

#@title Cloud image

# Join the array of strings into a single string
Context = " ".join(df["Context"])
Response = " ".join(df["Response"])

# Generate and display a word cloud image (Context)
wordcloud = WordCloud(background_color='white').generate(Context)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Context")
plt.show()

# Generate and display a word cloud image (Response)
wordcloud = WordCloud(background_color='white').generate(Response)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Response")
print(" ")
plt.show()

#@title Most common words

# Split the 'Context' documents into words and flatten the list
context_words = [word for document in df["Context"] for word in str(document).split()]

# Count the occurrences of each word in 'Context'
context_word_counts = Counter(context_words)

# Get the most common words and their counts in 'Context'
most_common_context_words = context_word_counts.most_common(10)

# Unzip the words and their counts in 'Context'
context_words, context_counts = zip(*most_common_context_words)

# Create and display a bar chart for 'Context'
plt.bar(context_words, context_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Context")
plt.show()

# Split the 'Response' documents into words and flatten the list
response_words = [word for document in df["Response"] for word in str(document).split()]

# Count the occurrences of each word in 'Response'
response_word_counts = Counter(response_words)

# Get the most common words and their counts in 'Response'
most_common_response_words = response_word_counts.most_common(10)

# Unzip the words and their counts in 'Response'
response_words, response_counts = zip(*most_common_response_words)

# Create and display a bar chart for 'Response'
plt.bar(response_words, response_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Response")
print(" ")
plt.show()

"""# Data 2
https://www.kaggle.com/datasets/thedevastator/mental-health-chatbot-pairs

## Explore data
"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_id = '1dDdJjepjJ8TEXkJZdjDVMGL0bKM7TlII' # id from you google drive file

download = drive.CreateFile({'id': file_id})

# Download the file to a local disc
download.GetContentFile('train2.csv')
df2  = pd.read_csv("train2.csv")
df2.head()

df2.shape

df2.info()

df2.isnull().sum()

"""## Clean data"""

#@title Remove the duplicate rows
df2.drop_duplicates()
df2.shape

df2["text"][0]

#@title Normalize Text
# & Remove Unicode Characters

def clean_text(text):
  text = str(text).lower() # Convert text to lowercase
  text = re.sub("[\[].*?[\]]", "", text) # Remove text within square brackets
  text = re.sub(r"http\S+", "", text) # Remove URLs
  text = re.sub(r"[^\w\s]", "", text) # Remove non-alphanumeric characters (excluding spaces)
  text = re.sub(r"\d+", "", text) # Remove digits
  return text

# Apply the clean_text function
df2['text'].apply(clean_text)
df2.head()

"""## Change token"""

#@title Replace function

# function that replace occurrences of <HUMAN> with <user> and <ASSISTANT> with <agent>
def replace_entities(text):
    # Replace <HUMAN> with <user>
    text = text.replace('<HUMAN>', '<user>')
    # Replace <ASSISTANT> with <agent>
    text = text.replace('<ASSISTANT>', '<agent>')
    # Replace : with space
    text = text.replace(':', '')
    return text

# Apply the function to each cell in the 'Context' column
df2['text'] = df2['text'].apply(replace_entities)

df2.head()

#@title Split to 2 columns

# Split each row into two columns based on the occurrence of the word "\n"
df2[['Context', 'Response']] = df2['text'].str.split('\n', 1, expand=True)

# Remove any leading or trailing whitespace from the 'Response' column
df2['Response'] = df2['Response'].str.strip()

df2.head()

#@title Remove escape sequences

df2['Context'] = df2['Context'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())
df2['Response'] = df2['Response'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())

df2.head()

#@title Drop text column
df2.drop('text', axis=1, inplace=True)
df2.head()

df2['Context'][0]

df2['Response'][0]

"""---------------

## Visualizations
"""

#@title Cloud image

# Join the array of strings into a single string
Context = " ".join(df2["Context"])
Response = " ".join(df2["Response"])

# Generate and display a word cloud image (Context)
wordcloud = WordCloud(background_color='white').generate(Context)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Context")
plt.show()

# Generate and display a word cloud image (Response)
wordcloud = WordCloud(background_color='white').generate(Response)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Response")
print(" ")
plt.show()

#@title Most common words

# Split the 'Context' documents into words and flatten the list
context_words = [word for document in df2["Context"] for word in str(document).split()]

# Count the occurrences of each word in 'Context'
context_word_counts = Counter(context_words)

# Get the most common words and their counts in 'Context'
most_common_context_words = context_word_counts.most_common(10)

# Unzip the words and their counts in 'Context'
context_words, context_counts = zip(*most_common_context_words)

# Create and display a bar chart for 'Context'
plt.bar(context_words, context_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Context")
plt.show()

# Split the 'Response' documents into words and flatten the list
response_words = [word for document in df2["Response"] for word in str(document).split()]

# Count the occurrences of each word in 'Response'
response_word_counts = Counter(response_words)

# Get the most common words and their counts in 'Response'
most_common_response_words = response_word_counts.most_common(10)

# Unzip the words and their counts in 'Response'
response_words, response_counts = zip(*most_common_response_words)

# Create and display a bar chart for 'Response'
plt.bar(response_words, response_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Response")
print(" ")
plt.show()

"""# Data 3
https://huggingface.co/datasets/mpingale/mental-health-chat-dataset?p=2

## Explore data
"""

!pip install datasets

#@title Load data for Hugging face
from datasets import load_dataset

dataset = load_dataset("mpingale/mental-health-chat-dataset")

#@title Convert to Pandas df

train_dataset = dataset["train"]
df3 = train_dataset.to_pandas()

df3.head()

#@title Drop columns

# Select only the 'questionText' and 'answerText' columns
df3 = df3[['questionText', 'answerText']]
df3.head()

df3.shape

df3.info()

df3.isnull().sum()

"""## Clean data"""

#@title Drop null rows
null_rows = df3.isnull().any(axis=1) # Identify rows with null values
df3 = df3[~null_rows] # Drop rows with null values

df3.shape # Display new shape

#@title Remove the duplicate rows
df3.drop_duplicates()
df3.shape

#@title Normalize Text
# & Remove Unicode Characters

def clean_text(text):
  text = str(text).lower() # Convert text to lowercase
  text = re.sub("[\[].*?[\]]", "", text) # Remove text within square brackets
  text = re.sub(r"http\S+", "", text) # Remove URLs
  text = re.sub(r"[^\w\s]", "", text) # Remove non-alphanumeric characters (excluding spaces)
  text = re.sub(r"\d+", "", text) # Remove digits
  return text

# Apply the clean_text function
df3['questionText'].apply(clean_text)
df3['answerText'].apply(clean_text)
df3.head()

"""## Add token"""

df3['questionText'] = df3['questionText'].apply(lambda x: "<user> " + str(x))
df3['answerText'] = df3['answerText'].apply(lambda x: "<agent> " + str(x))

df3.head()

#@title Remove escape sequences

df3['questionText'] = df3['questionText'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())
df3['answerText'] = df3['answerText'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())

df3.head()

#@title Rename Column

df3.rename(columns={'questionText': 'Context', 'answerText': 'Response'}, inplace=True)
df3.head()

df3['Context'][0]

df3['Response'][0]

"""## Visualizations"""

#@title Cloud image

# Join the array of strings into a single string
Context = " ".join(df3["Context"])
Response = " ".join(df3["Response"])

# Generate and display a word cloud image (Context)
wordcloud = WordCloud(background_color='white').generate(Context)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Context")
plt.show()

# Generate and display a word cloud image (Response)
wordcloud = WordCloud(background_color='white').generate(Response)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Response")
print(" ")
plt.show()

"""----------------------"""

#@title Most common words

# Split the 'Context' documents into words and flatten the list
context_words = [word for document in df3["Context"] for word in str(document).split()]

# Count the occurrences of each word in 'Context'
context_word_counts = Counter(context_words)

# Get the most common words and their counts in 'Context'
most_common_context_words = context_word_counts.most_common(10)

# Unzip the words and their counts in 'Context'
context_words, context_counts = zip(*most_common_context_words)

# Create and display a bar chart for 'Context'
plt.bar(context_words, context_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Context")
plt.show()

# Split the 'Response' documents into words and flatten the list
response_words = [word for document in df3["Response"] for word in str(document).split()]

# Count the occurrences of each word in 'Response'
response_word_counts = Counter(response_words)

# Get the most common words and their counts in 'Response'
most_common_response_words = response_word_counts.most_common(10)

# Unzip the words and their counts in 'Response'
response_words, response_counts = zip(*most_common_response_words)

# Create and display a bar chart for 'Response'
plt.bar(response_words, response_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Response")
print(" ")
plt.show()

"""-------------------

# Data 4
https://huggingface.co/datasets/alexandreteles/mental-health-conversational-data

## Explore data
"""

dataset = load_dataset("alexandreteles/mental-health-conversational-data")

#@title Convert to Pandas df

train_dataset = dataset["train"]
df4 = train_dataset.to_pandas()

df4.head()

df4.shape

df4.info()

df3.isnull().sum()

#@title Drop columns

# Select only the 'questionText' and 'answerText' columns
df4 = df4[['Context', 'Response']]
df4.head()

"""## Clean data"""

#@title Remove the duplicate rows
df4.drop_duplicates()
df4.shape

#@title Normalize Text
# & Remove Unicode Characters

def clean_text(text):
  text = str(text).lower() # Convert text to lowercase
  text = re.sub("[\[].*?[\]]", "", text) # Remove text within square brackets
  text = re.sub(r"http\S+", "", text) # Remove URLs
  text = re.sub(r"[^\w\s]", "", text) # Remove non-alphanumeric characters (excluding spaces)
  text = re.sub(r"\d+", "", text) # Remove digits
  return text

# Apply the clean_text function
df4['Context'].apply(clean_text)
df4['Response'].apply(clean_text)
df4.head()

#@title Remove escape sequences

df4['Context'] = df4['Context'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())
df4['Response'] = df4['Response'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())

df4.head()

"""## Add token"""

df4['Context'] = df4['Context'].apply(lambda x: "<user> " + str(x))
df4['Response'] = df4['Response'].apply(lambda x: "<agent> " + str(x))

df4.head()

df4['Context'][0]

df4['Response'][0]

"""## Visualizations"""

#@title Cloud image

# Join the array of strings into a single string
Context = " ".join(df4["Context"])
Response = " ".join(df4["Response"])

# Generate and display a word cloud image (Context)
wordcloud = WordCloud(background_color='white').generate(Context)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Context")
plt.show()

# Generate and display a word cloud image (Response)
wordcloud = WordCloud(background_color='white').generate(Response)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Response")
print(" ")
plt.show()

#@title Most common words

# Split the 'Context' documents into words and flatten the list
context_words = [word for document in df4["Context"] for word in str(document).split()]

# Count the occurrences of each word in 'Context'
context_word_counts = Counter(context_words)

# Get the most common words and their counts in 'Context'
most_common_context_words = context_word_counts.most_common(10)

# Unzip the words and their counts in 'Context'
context_words, context_counts = zip(*most_common_context_words)

# Create and display a bar chart for 'Context'
plt.bar(context_words, context_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Context")
plt.show()

# Split the 'Response' documents into words and flatten the list
response_words = [word for document in df4["Response"] for word in str(document).split()]

# Count the occurrences of each word in 'Response'
response_word_counts = Counter(response_words)

# Get the most common words and their counts in 'Response'
most_common_response_words = response_word_counts.most_common(10)

# Unzip the words and their counts in 'Response'
response_words, response_counts = zip(*most_common_response_words)

# Create and display a bar chart for 'Response'
plt.bar(response_words, response_counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.title("Response")
print(" ")
plt.show()

"""-------------------

# Download data
"""

from google.colab import files

#@title Data1

df.to_csv('health_data_1.csv', index=False) # Save the DataFrame to a CSV file
files.download('health_data_1.csv')

#@title Data2

df2.to_csv('health_data_2.csv', index=False) # Save the DataFrame to a CSV file
files.download('health_data_2.csv')

#@title Data3

df3.to_csv('health_data_3.csv', index=False) # Save the DataFrame to a CSV file
files.download('health_data_3.csv')

#@title Data4

df4.to_csv('health_data_4.csv', index=False) # Save the DataFrame to a CSV file
files.download('health_data_4.csv')